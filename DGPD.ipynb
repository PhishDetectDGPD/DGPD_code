{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from keras_preprocessing import sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "oc_path = \"/home/qidai/phishing_DGPD/optional_chars.pkl\"\n",
    "f = open(oc_path, \"rb\")\n",
    "optional_chars =  pickle.load(f)\n",
    "print(optional_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain2vector(domain, optional_chars):\n",
    "    vector = []\n",
    "    for d in domain:\n",
    "        tmp = []\n",
    "        for c in d:\n",
    "            if c in optional_chars:\n",
    "                tmp.append(optional_chars[c])\n",
    "            else:\n",
    "                tmp.append(89)\n",
    "        vector.append(tmp)\n",
    "    vector = sequence.pad_sequences(vector, maxlen=255, padding = \"post\", truncating = \"post\", value = 0)\n",
    "    results = []\n",
    "    for url in vector:\n",
    "        cur = []\n",
    "        for i in range(len(url)):\n",
    "            temp = [0 for i in range(90)]\n",
    "            temp[url[i]] = 1\n",
    "            cur.append(temp)\n",
    "        results.append(cur)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_file(path):\n",
    "    reader = pd.read_csv(path)\n",
    "    url = reader.urls.to_numpy()\n",
    "    return url\n",
    "legitimate_url = get_csv_file(\"./data/URLGAN_legitimate_url.csv\")\n",
    "print(len(legitimate_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape)\n",
    "    if torch.cuda.is_available():\n",
    "        U = U.cuda()\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature = 0.1):\n",
    "    y = logits + sample_gumbel(logits.size())\n",
    "    return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "def gumbel_softmax(logits, latent_dim = 255, categorical_dim = 90, temperature=0.75, hard=False):\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    \n",
    "    if not hard:\n",
    "        return y.view(-1, latent_dim, categorical_dim)\n",
    "\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    y_hard = (y_hard - y).detach() + y\n",
    "    return y_hard.view(-1, latent_dim, categorical_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layer1 = nn.Linear(1000, 25*12)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1, 8, 3, (1, 1), padding=(0, 2), bias=False)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 8, 7, (3, 3), padding=(0, 2), bias=False)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 8, 9, (3, 3), padding=(4, 4), bias=False)\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 1, 11, (1, 1), padding=(4, 4), bias=False)\n",
    "        )\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1, 1, 1, (1, 1), padding=(0, 0), bias=False)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        in1 = self.layer1(x)\n",
    "        in2 = in1.reshape(in1.size(0),1, 25, 12)\n",
    "        h1 = self.layer2(in2)\n",
    "        h2 = self.layer3(h1)\n",
    "        h3 = self.layer4(h2)\n",
    "        h4 = self.layer5(h3)\n",
    "        h5 = self.layer6(h4)\n",
    "        out = gumbel_softmax(h5)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dscriminator(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(Dscriminator, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(4, 4, kernel_size=7, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(4, 4, kernel_size=9, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(4, 1, kernel_size=11, stride=2, padding=2)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(24, 10),\n",
    "            nn.Linear(10, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        out_cnn = self.layer1(x)\n",
    "        out_cnn = self.layer2(out_cnn)\n",
    "        out_cnn = self.layer3(out_cnn)\n",
    "        out_cnn = self.layer4(out_cnn)\n",
    "        out_cnn = out_cnn.reshape(out_cnn.size(0), -1)\n",
    "        out = self.fc(out_cnn)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_D = 0.00005\n",
    "learning_rate_G = 0.00001\n",
    "batch_size = 512\n",
    "num_epochs = 500\n",
    "lambda_term = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_epoch = 237\n",
    "\n",
    "# Generator\n",
    "Generator_model = Generator().to(device)\n",
    "# path_of_generator = \"./model/URLGAN/Generator_\"+str(model_epoch)\n",
    "# checkpoint = torch.load(path_of_generator)\n",
    "# Generator_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# Generator_model.train()\n",
    "Generator_optimizer = torch.optim.RMSprop(Generator_model.parameters(), lr=learning_rate_G)\n",
    "\n",
    "\n",
    "# Descriminator\n",
    "Dscriminator_model = Dscriminator().to(device)\n",
    "# path_of_dscriminator = \"./model/URLGAN/Dscriminator_\"+str(98)\n",
    "# checkpoint = torch.load(path_of_dscriminator)\n",
    "# Dscriminator_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# Dscriminator_model.train()\n",
    "Dscriminator_optimizer = torch.optim.RMSprop(Dscriminator_model.parameters(), lr=learning_rate_D)\n",
    "\n",
    "def reset_grad():\n",
    "    Generator_optimizer.zero_grad()\n",
    "    Dscriminator_optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    np.random.seed()\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "\n",
    "def calculate_gradient_penalty(real_images, fake_images):\n",
    "    eta = torch.FloatTensor(batch_size,1,1).uniform_(0,1).to(device)\n",
    "    interpolated = eta * real_images + ((1 - eta) * fake_images).to(device)\n",
    "    interpolated = Variable(interpolated, requires_grad=True)\n",
    "    prob_interpolated = Dscriminator_model(interpolated)\n",
    "    gradients = autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                            grad_outputs=torch.ones(\n",
    "                                prob_interpolated.size()).to(device),\n",
    "                            create_graph=True, retain_graph=True)[0]\n",
    "    grad_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_term\n",
    "    return grad_penalty\n",
    "\n",
    "def ReCharacterization(data,valid_chars):\n",
    "    data_char = []\n",
    "    key_list = list(valid_chars.keys())\n",
    "    val_list = list(valid_chars.values())\n",
    "    for domain in data:\n",
    "        domain_char = []\n",
    "        for char in domain:\n",
    "            if(char != 0):\n",
    "                domain_char.append(key_list[val_list.index(char)])\n",
    "            else:\n",
    "                break\n",
    "        data_char.append(\"\".join(domain_char))\n",
    "    return data_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = legitimate_url\n",
    "y_train = [0 for i in range(len(X_train))]\n",
    "\n",
    "total_step = len(X_train)//batch_size\n",
    "len_train = len(X_train)\n",
    "training_times = len_train//batch_size\n",
    "one = torch.FloatTensor([1]).to(device)\n",
    "mone = one * -1\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(training_times):\n",
    "\n",
    "        # ================================================================== #\n",
    "        #                      Train the discriminator                       #\n",
    "        # ================================================================== #\n",
    "        # real data loss\n",
    "        batch_x, batch_y = next_batch(batch_size, X_train, y_train)\n",
    "        batch_x = domain2vector(batch_x, optional_chars)\n",
    "        batch_x_real = torch.tensor(batch_x).to(device).float()\n",
    "        outputs = Dscriminator_model(batch_x_real)\n",
    "        loss_real = outputs.mean(0).view(1)\n",
    "\n",
    "        # fake data loss\n",
    "        batch_x = torch.randn(batch_size, 1000).to(device).float()\n",
    "        outputs = Generator_model(batch_x)\n",
    "        batch_x_fake = outputs\n",
    "        outputs = Dscriminator_model(outputs)\n",
    "        loss_fake = outputs.mean(0).view(1)\n",
    "#         with torch.backends.cudnn.flags(enabled=False):\n",
    "        gradient_penalty = calculate_gradient_penalty(batch_x_real, batch_x_fake)\n",
    "        d_loss = loss_real - loss_fake + gradient_penalty\n",
    "\n",
    "\n",
    "        # Backward and optimize\n",
    "        reset_grad()\n",
    "        loss_real.backward(one)\n",
    "        loss_fake.backward(mone)\n",
    "        gradient_penalty.backward()\n",
    "        Dscriminator_optimizer.step()\n",
    "\n",
    "\n",
    "        # ================================================================== #\n",
    "        #                          Train the Generator                       #\n",
    "        # ================================================================== #\n",
    "        for k in range(2):\n",
    "            batch_x = torch.randn(batch_size, 1000).to(device).float()\n",
    "            outputs = Generator_model(batch_x)\n",
    "            outputs = Dscriminator_model(outputs)\n",
    "            g_loss = outputs.mean(0).view(1)\n",
    "\n",
    "            # Backward and optimize\n",
    "            reset_grad()\n",
    "            g_loss.backward(one)\n",
    "            Generator_optimizer.step()\n",
    "\n",
    "        if i%20 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], loss_real: {:.4f}, loss_fake: {:.4f}, d_loss: {:.4f}, gradient_penalty: {:.4f}, g_loss: {:.4f}' \n",
    "                  .format(epoch, num_epochs, i+1, total_step, loss_real.item(), loss_fake.item(), d_loss.item(), gradient_penalty.item(), g_loss.item()))\n",
    "\n",
    "    if (epoch+1)%2 == 0:\n",
    "        batch_x = torch.randn(5, 1000).to(device).float()\n",
    "        outputs = Generator_model(batch_x)\n",
    "        outputs = torch.squeeze(outputs)\n",
    "        outputs = torch.argmax(outputs, dim=2)\n",
    "        domain = ReCharacterization(outputs, optional_chars)\n",
    "        print(domain)\n",
    "\n",
    "    if (epoch+1)%10 == 0:\n",
    "        state = {\n",
    "            'model_state_dict': Generator_model.state_dict(),\n",
    "        }\n",
    "        torch.save(state, \"./model/URLGAN/D_Generator1000_\"+str(epoch+model_epoch))\n",
    "        state = {\n",
    "            'model_state_dict': Dscriminator_model.state_dict(),\n",
    "        }\n",
    "        torch.save(state, \"./model/URLGAN/D_Dscriminator1000_\"+str(epoch+model_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_file(path):\n",
    "    reader = pd.read_csv(path)\n",
    "    url = reader.urls.to_numpy()\n",
    "    return url\n",
    "\n",
    "def get_txt_file(path):\n",
    "    f = open(path)\n",
    "    lines = f.readlines()\n",
    "    lines = eval(lines[0])\n",
    "    return lines\n",
    "    \n",
    "def get_csv_file_url(path):\n",
    "    reader = pd.read_csv(path)\n",
    "    url = reader.url.to_numpy()\n",
    "    return url\n",
    "\n",
    "phishing_url_2018 = get_csv_file(\"/home/qidai/phishing_DGPD/data/phishing_url_2018.csv\")\n",
    "phishing_url_2019 = get_csv_file(\"/home/qidai/phishing_DGPD/data/phishing_url_2019.csv\")\n",
    "phishing_url_2020 = get_csv_file(\"/home/qidai/phishing_DGPD/data/phishing_url_2020.csv\")\n",
    "phishing_url_2021 = get_csv_file(\"/home/qidai/phishing_DGPD/data/phishing_url_2021.csv\")\n",
    "phishing_url_2022 = get_csv_file(\"/home/qidai/phishing_DGPD/data/phishing_url_2022.csv\")\n",
    "\n",
    "legitimate_url_dgpd = get_csv_file(\"/home/qidai/phishing_DGPD/data/DGPD_legitimate_url.csv\")\n",
    "legitimate_url_url2vec = get_csv_file(\"/home/qidai/phishing_DGPD/data/URL2Vec_legitimate_url.csv\")\n",
    "legitimate_url_urlgan = get_csv_file(\"/home/qidai/phishing_DGPD/data/URLGAN_legitimate_url_origin.csv\")\n",
    "\n",
    "\n",
    "\n",
    "print(len(phishing_url_2018))\n",
    "print(len(phishing_url_2019))\n",
    "print(len(phishing_url_2020))\n",
    "print(len(phishing_url_2021))\n",
    "print(len(phishing_url_2022))\n",
    "print(len(legitimate_url_dgpd))\n",
    "print(len(legitimate_url_url2vec))\n",
    "print(len(legitimate_url_urlgan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain2vector(domain, optional_chars):\n",
    "    vector = []\n",
    "    for d in domain:\n",
    "        tmp = []\n",
    "        for c in d:\n",
    "            if c in optional_chars:\n",
    "                tmp.append(optional_chars[c])\n",
    "            else:\n",
    "                tmp.append(89)\n",
    "        vector.append(tmp)\n",
    "    vector = sequence.pad_sequences(vector, maxlen = 255, padding = \"post\", truncating = \"post\", value = 0)\n",
    "    return vector\n",
    "\n",
    "def domain2vector_ignore(domain, optional_chars):\n",
    "    np.random.seed()\n",
    "    vector = []\n",
    "    for x in domain:\n",
    "        cur = []\n",
    "        for y in x:\n",
    "            if y == \" \":\n",
    "                cur.append(0)\n",
    "            elif y in optional_chars:\n",
    "                cur.append(optional_chars[y])\n",
    "            else:\n",
    "                cur.append(89)\n",
    "        vector.append(cur)\n",
    "    vector = sequence.pad_sequences(vector, maxlen = 255, padding = \"post\", truncating = \"post\", value = 0)\n",
    "    return vector\n",
    "\n",
    "def add_mlm(data, rate = 0.15, optional_chars = optional_chars):\n",
    "    np.random.seed()\n",
    "    data_p = []\n",
    "    pos_p = []\n",
    "    for i in range(len(data)):\n",
    "        domain = data[i]\n",
    "        l = len(domain)\n",
    "        idx = np.arange(0, l)\n",
    "        np.random.shuffle(idx)\n",
    "        pos_p.append(idx[0:int(l*rate)])\n",
    "        for j in range(int(l*rate)):\n",
    "            uni = np.random.uniform(0, 1)\n",
    "            if uni < 0.6:\n",
    "                domain = domain[0:idx[j]] + ' ' + domain[idx[j]+1:]\n",
    "            elif uni < 0.8:\n",
    "                domain = domain[0:idx[j]] + list(optional_chars.keys())[np.random.randint(1, 89)] + domain[idx[j]+1:]\n",
    "        data_p.append(domain)\n",
    "    return data_p, pos_p\n",
    "\n",
    "def add_mask(data, rate = 0.15):\n",
    "    np.random.seed()\n",
    "    data_p = []\n",
    "    for i in range(len(data)):\n",
    "        domain = data[i]\n",
    "        l = len(domain)\n",
    "        idx = np.arange(0, l)\n",
    "        np.random.shuffle(idx)\n",
    "        for j in range(int(l*rate)):\n",
    "            domain = domain[0:idx[j]] + ' ' + domain[idx[j]+1:]\n",
    "        data_p.append(domain)\n",
    "    return data_p\n",
    "\n",
    "def add_random_char(data, rate = 0.15, optional_chars = optional_chars):\n",
    "    np.random.seed()\n",
    "    data_p = []\n",
    "    for i in range(len(data)):\n",
    "        domain = data[i]\n",
    "        l = len(domain)\n",
    "        idx = np.arange(0, l)\n",
    "        np.random.shuffle(idx)\n",
    "        for j in range(int(l*rate)):\n",
    "            domain = domain[0:idx[j]] + list(optional_chars.keys())[np.random.randint(1, 89)] + domain[idx[j]+1:]\n",
    "        data_p.append(domain)\n",
    "    return data_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = []\n",
    "training_set.extend(legitimate_url_url2vec[:2000000])\n",
    "\n",
    "training_set_mask = []\n",
    "training_label_mask = []\n",
    "\n",
    "data_p, pos_p = add_mlm(training_set, 0.15)\n",
    "training_set_mask.extend(data_p)\n",
    "training_label_mask.extend(training_set)\n",
    "\n",
    "for i in range(len(training_label_mask)):\n",
    "    cur = \"\"\n",
    "    for j in range(len(training_label_mask[i])):\n",
    "        if j not in pos_p[i]:\n",
    "            cur += \" \"\n",
    "        else:\n",
    "            cur += training_label_mask[i][j]\n",
    "    training_label_mask[i] = cur\n",
    "            \n",
    "training_set_mask = domain2vector(training_set_mask, optional_chars)\n",
    "training_label_mask = domain2vector_ignore(training_label_mask, optional_chars)\n",
    "\n",
    "\n",
    "test_set_mask = []\n",
    "test_label_mask = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1) / torch.pow(10000, torch.arange(\n",
    "            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.embedding = nn.Embedding(100,128)\n",
    "        self.pos_encoding = PositionalEncoding(128, 0)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8, dropout=0.2, dim_feedforward=256)\n",
    "        self.trm_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.output = nn.Linear(128,90)\n",
    "\n",
    "    def forward(self,x):\n",
    "        h1 = self.pos_encoding(self.embedding(x)).permute(1, 0, 2)\n",
    "        h2 = self.trm_encoder(h1)\n",
    "        output = self.output(h2).permute(1, 0, 2)\n",
    "        return output\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    '''Return a total of `num` random samples and labels. '''\n",
    "    np.random.seed()\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 400\n",
    "batch_size = 256\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Descriminator\n",
    "model = Net().to(device)\n",
    "# path_of_model = \"./model/URL2Vec/DQ_URL2Vec\"\n",
    "# checkpoint = torch.load(path_of_model)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# model.train()\n",
    "model_criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "X_train = training_set_mask\n",
    "y_train = training_label_mask\n",
    "\n",
    "total_step = len(X_train)//batch_size\n",
    "len_train = len(X_train)\n",
    "training_time = len_train//batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_rec = []\n",
    "    for i in range(training_time):\n",
    "\n",
    "        batch_x, batch_y = next_batch(batch_size, X_train, y_train)\n",
    "        batch_x = torch.LongTensor(batch_x).to(device)\n",
    "        batch_y = torch.LongTensor(batch_y).to(device).reshape(-1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_x).reshape(-1, 90)\n",
    "        loss = model_criterion(outputs, batch_y)\n",
    "        loss_rec.append(loss.item())\n",
    "        \n",
    "        # Backward and optimize\n",
    "        model_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        model_optimizer.step()\n",
    "\n",
    "    if(epoch%1 == 0):\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.12f}'.format(epoch+1, num_epochs, i+1, total_step, np.mean(loss_rec)))\n",
    "        state = {'model_state_dict': model.state_dict()}\n",
    "        torch.save(state, \"./model/DQ_URL2Vec_\"+str(epoch+26))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain2vector(domain, optional_chars):\n",
    "    vector = []\n",
    "    for d in domain:\n",
    "        tmp = []\n",
    "        for c in d:\n",
    "            if c in optional_chars:\n",
    "                tmp.append(optional_chars[c])\n",
    "            else:\n",
    "                tmp.append(89)\n",
    "        vector.append(tmp)\n",
    "    vector = sequence.pad_sequences(vector, maxlen = 255, padding = \"post\", truncating = \"post\", value = 0)\n",
    "    return vector\n",
    "\n",
    "def domain2vector_ignore(domain, optional_chars):\n",
    "    np.random.seed()\n",
    "    vector = []\n",
    "    for x in domain:\n",
    "        cur = []\n",
    "        for y in x:\n",
    "            if y == \" \":\n",
    "                cur.append(0)\n",
    "            elif y in optional_chars:\n",
    "                cur.append(optional_chars[y])\n",
    "            else:\n",
    "                cur.append(89)\n",
    "        vector.append(cur)\n",
    "    vector = sequence.pad_sequences(vector, maxlen = 255, padding = \"post\", truncating = \"post\", value = 0)\n",
    "    return vector\n",
    "\n",
    "def add_mlm(data, rate = 0.15, optional_chars = optional_chars):\n",
    "    np.random.seed()\n",
    "    data_p = []\n",
    "    pos_p = []\n",
    "    for i in range(len(data)):\n",
    "        domain = data[i]\n",
    "        l = len(domain)\n",
    "        idx = np.arange(0, l)\n",
    "        np.random.shuffle(idx)\n",
    "        pos_p.append(idx[0:int(l*rate)])\n",
    "        for j in range(int(l*rate)):\n",
    "            uni = np.random.uniform(0, 1)\n",
    "            if uni < 0.6:\n",
    "                domain = domain[0:idx[j]] + ' ' + domain[idx[j]+1:]\n",
    "            elif uni < 0.8:\n",
    "                domain = domain[0:idx[j]] + list(optional_chars.keys())[np.random.randint(1, 89)] + domain[idx[j]+1:]\n",
    "        data_p.append(domain)\n",
    "    return data_p, pos_p\n",
    "\n",
    "def add_mask(data, rate = 0.15):\n",
    "    np.random.seed()\n",
    "    data_p = []\n",
    "    for i in range(len(data)):\n",
    "        domain = data[i]\n",
    "        l = len(domain)\n",
    "        idx = np.arange(0, l)\n",
    "        np.random.shuffle(idx)\n",
    "        for j in range(int(l*rate)):\n",
    "            domain = domain[0:idx[j]] + ' ' + domain[idx[j]+1:]\n",
    "        data_p.append(domain)\n",
    "    return data_p\n",
    "\n",
    "def add_random_char(data, rate = 0.15, optional_chars = optional_chars):\n",
    "    np.random.seed()\n",
    "    data_p = []\n",
    "    for i in range(len(data)):\n",
    "        domain = data[i]\n",
    "        l = len(domain)\n",
    "        idx = np.arange(0, l)\n",
    "        np.random.shuffle(idx)\n",
    "        for j in range(int(l*rate)):\n",
    "            domain = domain[0:idx[j]] + list(optional_chars.keys())[np.random.randint(1, 89)] + domain[idx[j]+1:]\n",
    "        data_p.append(domain)\n",
    "    return data_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1) / torch.pow(10000, torch.arange(\n",
    "            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(100,128)\n",
    "        self.pos_encoding = PositionalEncoding(128, 0)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8, dropout=0.2, dim_feedforward=256)\n",
    "        self.trm_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.output = nn.Linear(128,90)\n",
    "        self.l1 = nn.Linear(90,32)\n",
    "        self.l2 = nn.Linear(32, 1)\n",
    "        self.l3 = nn.ReLU()\n",
    "        \n",
    "        self.l4 = nn.Linear(255, 64)\n",
    "        self.l5 = nn.Linear(64, 1)\n",
    "        self.l6 = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        h0 = self.embedding(x)\n",
    "        h1 = self.pos_encoding(h0).permute(1, 0, 2)\n",
    "        h2 = self.trm_encoder(h1)\n",
    "        o0 = self.output(h2).permute(1, 0, 2)\n",
    "        o1 = self.l3(self.l1(o0))\n",
    "        o2 = self.l3(self.l2(o1))\n",
    "        output = self.l6(self.l5(self.l4(o2.reshape(-1, 255))))\n",
    "        return output\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    np.random.seed()\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = []\n",
    "training_set_p = []\n",
    "training_label_p = []\n",
    "training_set.extend(legitimate_url_dgpd[0:160000])\n",
    "training_label = [0 for i in range(len(training_set))]\n",
    "\n",
    "training_set_p.extend(add_mask(training_set, 0.15))\n",
    "training_label_p.extend(training_label)\n",
    "training_set_p.extend(add_random_char(training_set, 0.15))\n",
    "training_label_p.extend(training_label)\n",
    "\n",
    "training_set.extend(training_set_p)\n",
    "training_label.extend(training_label_p)\n",
    "\n",
    "training_set.extend(phishing_url_2018[0:160000])\n",
    "training_label.extend([1 for i in range(len(training_set)-len(training_label))])\n",
    "\n",
    "training_set = domain2vector(training_set, optional_chars)\n",
    "training_label = training_label\n",
    "\n",
    "test_set_2018 = []\n",
    "test_set_2018.extend(legitimate_url_dgpd[160000:])\n",
    "test_label_2018 = [0 for i in range(len(test_set_2018))]\n",
    "test_set_2018.extend(phishing_url_2018[160000:])\n",
    "test_label_2018.extend([1 for i in range(len(test_set_2018)-len(test_label_2018))])\n",
    "test_set_2018 = domain2vector(test_set_2018, optional_chars)\n",
    "\n",
    "test_set_2019 = []\n",
    "test_set_2019.extend(legitimate_url_dgpd[160000:])\n",
    "test_label_2019 = [0 for i in range(len(test_set_2019))]\n",
    "test_set_2019.extend(phishing_url_2019[160000:])\n",
    "test_label_2019.extend([1 for i in range(len(test_set_2019)-len(test_label_2019))])\n",
    "test_set_2019 = domain2vector(test_set_2019, optional_chars)\n",
    "\n",
    "test_set_2020 = []\n",
    "test_set_2020.extend(legitimate_url_dgpd[160000:])\n",
    "test_label_2020 = [0 for i in range(len(test_set_2020))]\n",
    "test_set_2020.extend(phishing_url_2020[160000:])\n",
    "test_label_2020.extend([1 for i in range(len(test_set_2020)-len(test_label_2020))])\n",
    "test_set_2020 = domain2vector(test_set_2020, optional_chars)\n",
    "\n",
    "test_set_2021 = []\n",
    "test_set_2021.extend(legitimate_url_dgpd[160000:])\n",
    "test_label_2021 = [0 for i in range(len(test_set_2021))]\n",
    "test_set_2021.extend(phishing_url_2021[160000:])\n",
    "test_label_2021.extend([1 for i in range(len(test_set_2021)-len(test_label_2021))])\n",
    "test_set_2021 = domain2vector(test_set_2021, optional_chars)\n",
    "\n",
    "test_set_2022 = []\n",
    "test_set_2022.extend(legitimate_url_dgpd[160000:])\n",
    "test_label_2022 = [0 for i in range(len(test_set_2022))]\n",
    "test_set_2022.extend(phishing_url_2022[160000:])\n",
    "test_label_2022.extend([1 for i in range(len(test_set_2022)-len(test_label_2022))])\n",
    "test_set_2022 = domain2vector(test_set_2022, optional_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 80\n",
    "batch_size = 128\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Descriminator\n",
    "model = Classifier().to(device)\n",
    "model.train()\n",
    "model_criterion = nn.BCELoss()\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in model.parameters():\n",
    "    para.requires_grad = False\n",
    "model.l1.weight.requires_grad = True\n",
    "model.l1.bias.requires_grad = True\n",
    "model.l2.weight.requires_grad = True\n",
    "model.l2.bias.requires_grad = True\n",
    "model.l4.weight.requires_grad = True\n",
    "model.l4.bias.requires_grad = True\n",
    "model.l5.weight.requires_grad = True\n",
    "model.l5.bias.requires_grad = True\n",
    "model.output.weight.requires_grad = True\n",
    "model.output.bias.requires_grad = True\n",
    "\n",
    "for para in model.trm_encoder.layers[5].parameters():\n",
    "    para.requires_grad = True\n",
    "\n",
    "X_train = training_set\n",
    "y_train = training_label\n",
    "\n",
    "total_step = len(X_train)//batch_size\n",
    "len_train = len(X_train)\n",
    "training_time = len_train//batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_rec = []\n",
    "    for i in range(training_time):\n",
    "\n",
    "        batch_x, batch_y = next_batch(batch_size, X_train, y_train)\n",
    "        batch_x = torch.LongTensor(batch_x).to(device)\n",
    "        batch_y = torch.Tensor(batch_y).to(device)\n",
    "        batch_y = torch.unsqueeze(batch_y,1)\n",
    "        \n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        loss = model_criterion(outputs, batch_y)\n",
    "        loss_rec.append(loss.item())\n",
    "        \n",
    "        # Backward and optimize\n",
    "        model_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        model_optimizer.step()\n",
    "\n",
    "    if(epoch%1 == 0):\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.12f}'.format(epoch+1, num_epochs, i+1, total_step, np.mean(loss_rec)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qidai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
